{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing and data extraction to npy files for model training",
   "id": "c8accf8cf633dc44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf"
   ],
   "id": "5b27431213330b09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gives the bbox and label for n-th box",
   "id": "61b96b04da99a14b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def read_string(ref, file_ref):\n",
    "    return \"\".join([chr(c[0]) for c in file_ref[ref]])\n",
    "\n",
    "def read_string(ref):\n",
    "    return \"\".join([chr(c[0]) for c in f[ref]])\n",
    "\n",
    "def get_name(i, name_refs):\n",
    "    name_ref = name_refs[i-1][0]\n",
    "    return read_string(name_ref)\n",
    "\n",
    "def get_boxes(img_num, f, bbox_refs, name_refs, path):\n",
    "    master_ref = bbox_refs[img_num-1][0]\n",
    "    bbox_data_struct = f[master_ref]\n",
    "    label_refs = bbox_data_struct[\"label\"]\n",
    "    num_of_digits = label_refs.shape[0]\n",
    "\n",
    "    def get_field(field, digit_num):\n",
    "        field_refs = bbox_data_struct[field]\n",
    "\n",
    "        # if only 1 digit in img then refrence is to float\n",
    "        if (field_refs.dtype == np.dtype(\"float64\")):\n",
    "            field_value = field_refs[0][0].astype(\"int16\")\n",
    "        else: # otherwise have to get the that the specific digit_num\n",
    "            field_value = f[field_refs[digit_num][0]][0][0].astype(\"int16\")\n",
    "\n",
    "        return field_value\n",
    "\n",
    "    bboxes_data = []\n",
    "    label_data = []\n",
    "    filepath = path + get_name(img_num, name_refs)\n",
    "    width, height = Image.open(filepath).size\n",
    "    for i in range(num_of_digits):\n",
    "        '''\n",
    "        digit_dict = {\n",
    "            \"left\": get_field(\"left\", i)/max_length,\n",
    "            \"top\": get_field(\"top\", i)/max_length,\n",
    "            \"height\": get_field(\"height\", i)/max_length,\n",
    "            \"width\": get_field(\"width\", i)/max_length,\n",
    "#            \"label\": get_field(\"label\", i),\n",
    "        }\n",
    "        '''\n",
    "        if height > width:\n",
    "            x = (get_field(\"left\", i) + (height - width) / 2)/ height\n",
    "            y = get_field(\"top\", i) / height\n",
    "            box_height = get_field(\"height\", i) / height\n",
    "            box_width = get_field(\"width\", i) / height\n",
    "        else:\n",
    "            x = get_field(\"left\", i) / width\n",
    "            y = (get_field(\"top\", i) + (width - height) / 2) / width\n",
    "            box_height = get_field(\"height\", i) / width\n",
    "            box_width = get_field(\"width\", i) / width\n",
    "        bbox_data = np.array([x, y, box_height, box_width], dtype='float32')\n",
    "        bboxes_data.append(bbox_data)\n",
    "        label_data.append(get_field(\"label\", i))\n",
    "    bbox_data = np.array([0,0,0,0], dtype='float32')\n",
    "    while len(bboxes_data) < 6:\n",
    "        bboxes_data.append(bbox_data)\n",
    "        label_data.append(0)\n",
    "    return bboxes_data, label_data\n",
    "\n",
    "max_digits = 0\n",
    "'''\n",
    "for i in range(1, len(names_refs) + 1):\n",
    "    data =  get_boxes(i)\n",
    "    if len(data) > max_digits:\n",
    "        max_digits = len(data)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "bbox_num = 1\n",
    "filename = get_name(bbox_num)\n",
    "box_info, label_info = get_boxes(bbox_num)\n",
    "print(filename, box_info, label_info)\n",
    "width2, height2 = Image.open('data/train/1.png').size\n",
    "print(width2, height2)\n",
    "\n",
    "\n",
    "max_digits # oli 6\n"
   ],
   "id": "d711c476910970e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Saves image data",
   "id": "bac8d634234fedb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nrOfPictures_train = 33402\n",
    "nrOfPictures_extra = 202353\n",
    "total = nrOfPictures_train + nrOfPictures_extra"
   ],
   "id": "73bd5c72fade1f4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "def save_img_data(source_dir, n, list, idx):\n",
    "    for i in range(1,n+1):\n",
    "        name = str(i) + \".png\"\n",
    "        image = Image.open(source_dir + name)\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.image.resize_with_pad(image, 128, 128)\n",
    "        image_array = np.array(image, dtype='float32') / 255\n",
    "        list[idx] = image_array\n",
    "        idx = idx + 1\n",
    "        if (i % 1000 == 0):\n",
    "            print(i)\n",
    "    return idx\n",
    "\n",
    "\n"
   ],
   "id": "13ec35f7fe4a8d3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Saves label data",
   "id": "e139e2195edecbae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "picture_data = np.empty(shape=(total,128,128,1), dtype='float32')\n",
    "index = save_img_data(\"data/train/\", nrOfPictures_train, picture_data, 0)\n",
    "print(\"DONE-1\")\n",
    "index = save_img_data(\"data/extra/\", nrOfPictures_extra, picture_data, index)\n",
    "print(\"DONE-2\")\n",
    "np.savez_compressed(\"data/X_train\", picture_data)\n",
    "picture_data = None\n"
   ],
   "id": "41740705f37ead92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def save_labels(file, n, box_list, label_list, idx, path):\n",
    "    bbox_refs = file[\"digitStruct\"][\"bbox\"]\n",
    "    name_refs = file[\"digitStruct\"][\"name\"]\n",
    "    for i in range(1,n+1):\n",
    "        bboxes, labels = get_boxes(i, file, bbox_refs, name_refs, path)\n",
    "        box_list[idx] = bboxes\n",
    "        label_list[idx] = labels\n",
    "        idx = idx + 1\n",
    "        if (i % 1000 == 0):\n",
    "            print(i)\n",
    "    return idx\n",
    "\n",
    "bbox_array = np.empty(shape=(total, 6, 4))\n",
    "labels_array = np.empty(shape=(total, 6))\n",
    "f = h5py.File(\"data/train_digitStruct.mat\", \"r\")\n",
    "index = save_labels(f, nrOfPictures_train, bbox_array, labels_array, 0, 'data/train/')\n",
    "f.close()\n",
    "f = h5py.File(\"data/extra_digitStruct.mat\", \"r\")\n",
    "index = save_labels(f, nrOfPictures_extra, bbox_array, labels_array, index, 'data/extra/')\n",
    "\n",
    "np.savez_compressed(\"data/bboxes_train\", bbox_array)\n",
    "np.savez_compressed(\"data/labels_train\", labels_array)\n",
    "#np.savez_compressed(\"data/labels_train_2\", np.array(labels_array))\n",
    "\n"
   ],
   "id": "795ae629b2d77d5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d4ae84229c313e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "bbox_array = None\n",
    "labels_array = None\n",
    "bbox_refs = None\n",
    "f.close()"
   ],
   "id": "3e39601448b8a0a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This displays images and the bounding boxes of the digits (1-500 checked), bad images in bad_data",
   "id": "97945c2bcfc12d0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "plt.close(\"all\")\n",
    "\n",
    "def visualize_fixed_bboxes(image_dir, y_train, num_show, rows, cols):\n",
    "    \"\"\"\n",
    "    Loads images and displays them with matplotlib bounding boxes, handling\n",
    "    NORMALIZED [Left, Top, Height, Width] format by swapping H and W for plotting.\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n",
    "        key=lambda x: int(os.path.splitext(x)[0]) if os.path.splitext(x)[0].isdigit() else x\n",
    "    )\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Error: No images found in {image_dir}\")\n",
    "        return\n",
    "\n",
    "    num_show = min(num_show, len(image_files), len(y_train))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_show):\n",
    "        try:\n",
    "            # 1. Load the Image and get dimensions\n",
    "            img_path = os.path.join(image_dir, image_files[i])\n",
    "            image = Image.open(img_path)\n",
    "            image_array = np.array(image)\n",
    "\n",
    "            ax = axes[i]\n",
    "\n",
    "            ax.imshow(image_array)\n",
    "\n",
    "            boxes_for_image = y_train[i]\n",
    "\n",
    "            width, height = image.size\n",
    "            max_length = max(width, height)\n",
    "\n",
    "            # --- Calculate Padding Offsets ---\n",
    "            # These offsets are needed to convert from the 'squared' normalized space\n",
    "            # back into the original image's coordinate system.\n",
    "            pad_x = 0\n",
    "            pad_y = 0\n",
    "\n",
    "            if height > width:\n",
    "                # Image was padded horizontally (left/right)\n",
    "                pad_x = (height - width) / 2\n",
    "            elif width > height:\n",
    "                # Image was padded vertically (top/bottom)\n",
    "                pad_y = (width - height) / 2\n",
    "\n",
    "            # The normalization factor is the max dimension\n",
    "            # max_length is already correctly calculated as max(width, height)\n",
    "\n",
    "            for bbox in boxes_for_image:\n",
    "                # The format in y_train is: [L, T, H, W]\n",
    "                normalized_left, normalized_top, normalized_height, normalized_width = bbox\n",
    "\n",
    "                if np.sum(bbox) == 0:\n",
    "                    continue\n",
    "\n",
    "                # --- RESCALE COORDINATES AND REMOVE PADDING OFFSET ---\n",
    "\n",
    "                # 1. Rescale by multiplying by the normalization factor (max_length)\n",
    "                left_rescaled = normalized_left * max_length\n",
    "                top_rescaled = normalized_top * max_length\n",
    "\n",
    "                # 2. Subtract the padding offset to get coordinates in the original image space\n",
    "                left_pixel = left_rescaled - pad_x\n",
    "                top_pixel = top_rescaled - pad_y\n",
    "\n",
    "                # 3. Rescale Dimensions (dimensions do not have padding offset applied)\n",
    "                pixel_width = normalized_width * max_length\n",
    "                pixel_height = normalized_height * max_length\n",
    "\n",
    "                # 4. Create the Matplotlib Bounding Box Patch (W, H are already correct from y_train)\n",
    "                # Matplotlib requires (Left, Top, Width, Height)\n",
    "                rect = patches.Rectangle(\n",
    "                    (left_pixel, top_pixel), # Top-Left corner (NOW CORRECTED)\n",
    "                    pixel_width,             # Width\n",
    "                    pixel_height,            # Height                    linewidth=2,\n",
    "                    edgecolor='blue',\n",
    "                    facecolor='none'\n",
    "                )\n",
    "\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            ax.set_title(f\"Image: {image_files[i]}\\n(L, T, H, W format)\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {image_files[i]}: {e}\")\n",
    "\n",
    "    # Hide any unused subplots if num_show is less than ROWS * COLS\n",
    "    for j in range(num_show, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "NUM_IMAGES_TO_SHOW = 20\n",
    "IMAGE_DIR = 'data/train/'\n",
    "COLS = 5\n",
    "ROWS = int(np.ceil(NUM_IMAGES_TO_SHOW / COLS))\n",
    "y_train = np.load(\"data/y_train.npy\", allow_pickle=True)\n",
    "print(f\"Attempting to display {NUM_IMAGES_TO_SHOW} images from {IMAGE_DIR} with normalized bboxes...\")\n",
    "visualize_fixed_bboxes(IMAGE_DIR, y_train, NUM_IMAGES_TO_SHOW, ROWS, COLS)"
   ],
   "id": "f42ca96c8c1d33c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Manual label verification is very tedious and not feasible for 30k images in train/ folder, so we thought that we could filter the pictures somewhat by using an trained model's predictions it got wrong, so it would make the workload smaller, we used the images in train/ folder.",
   "id": "b50a5f52462a27d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import keras\n",
    "model_path = 'models/svhnModel.keras'\n",
    "\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "\n",
    "data_X_train = np.load(\"data/X_train.npy\", allow_pickle=True)\n",
    "data_y_train = np.load(\"data/y_train.npy\", allow_pickle=True)\n",
    "data_labels_train = np.load(\"data/labels_train.npy\", allow_pickle=True)\n",
    "\n",
    "print(data_X_train.shape)\n",
    "print(data_y_train.shape)\n",
    "print(data_labels_train.shape)\n",
    "\n",
    "ds_data = tf.data.Dataset.from_tensor_slices((data_X_train, {\"bboxes\": data_y_train, \"classes\": data_labels_train}))\n",
    "\n",
    "ds_data = ds_data.batch(64).prefetch(tf.data.AUTOTUNE)\n"
   ],
   "id": "adf3da726745c730"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = loaded_model.evaluate(ds_data)\n",
    "predictions = loaded_model.predict(ds_data)\n",
    "print(results)\n",
    "print('Test loss: %.4f accuracy: %.4f' % (results[2], results[4]))"
   ],
   "id": "32cb1e98049fa916"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bboxes_pred = predictions[0]  # Predicted BBoxes (N, 6, 4)\n",
    "classes_pred = predictions[1] # Predicted Class Probabilities (N, 6, 11)\n",
    "\n",
    "y_pred_classes = np.argmax(classes_pred, axis=2)\n",
    "\n",
    "y_true_classes = data_labels_train\n",
    "\n",
    "is_predicted_digit = (y_pred_classes != 0)\n",
    "\n",
    "predicted_digit_counts = np.sum(is_predicted_digit, axis=1)\n",
    "is_true_digit = (y_true_classes != 0)\n",
    "true_digit_counts = np.sum(is_true_digit, axis=1) # Shape: (N,)\n",
    "\n",
    "is_miscounted = (predicted_digit_counts != true_digit_counts)\n",
    "\n",
    "mismatches = (y_pred_classes != y_true_classes)\n",
    "\n",
    "is_image_misclassified = np.any(mismatches, axis=1)\n",
    "\n",
    "counter = 0\n",
    "for i in range(predictions[1].shape[0]):\n",
    "    if is_image_misclassified[i] and is_miscounted[i] and true_digit_counts[i] < predicted_digit_counts[i]:\n",
    "        # print(str(i+1) + \".png\")\n",
    "        # print(f'pred: {y_pred_classes[i]}, real: {y_true_classes[i]}')\n",
    "        counter += 1\n",
    "print(data_labels_train[36])\n",
    "print(y_pred_classes[36])\n",
    "print(\"Number of all images:\", predictions[1].shape[0])\n",
    "print(\"How many images to sift through (not correct because of overfitting:\", counter)\n",
    "print(\"Accuracy:\", 1 - (counter / predictions[1].shape[0]))"
   ],
   "id": "c5c5cf53aa91b9d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The problem came with overfitting (for example 37.png in train/, which should be the number 121, but because the label is 11 the model predicts it as such as well), technically we can't have access to the test dataset, so we had to check the validty of the training data with a model trained on that data, but this didn't\n",
    " yield any results obviously because there were many cases where the labels were incorrect, but the model still predicted it to be correct. One way to fix this is to split the training data for example in half, and validate the second half, but our models accuracy still wasn't good enough for this (in part because of the bad data), and halving the training data will surely make it worse. In a way it is a chicken and egg problem. One way to fix this problem is introducing extra data from extra/\n",
    " to make the model more accurate to less data to sift through. Sadly this would also mean the absolute number of images to verify increases as well. Finally, we did not use update bad_data in the end because of these reasons."
   ],
   "id": "b6208ad984e2571"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
